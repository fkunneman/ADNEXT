#! /usr/bin/env python

from __future__ import division
import re
import proy_evaluation 
import time_functions
from collections import defaultdict
import datetime
import matplotlib.pyplot as plt
from sklearn.metrics import auc
import codecs

class Evalset():

    def __init__(self):
        self.instances = []
        self.name_instance = {}
        self.testset_instances = defaultdict(list)
        self.time_buckets = defaultdict(list)

    def set_meta(self,metafile,multiple=False):
        meta = codecs.open(metafile,"r","utf-8")
        metaread = meta.readlines()
        meta.close()
        for line in metaread:
            tokens = line.split("\t")
            instance = Evalset.Instance()
            instance.set_id(tokens[0])
            instance.set_event(tokens[1])
            instance.set_label(tokens[2])
            instance.set_tfz(int(tokens[3]))
            self.instances.append(instance)
            if multiple:
                self.testset_instances[instance.event].append(instance)
                
    def calculate_mrse(self,instances,outfile,relative = True):
        stepwise_evaluation = []
        estimations = 0
        squared_error = 0
        for instance in instances:
            target = instance.label
            prediction = instance.classification
            evaluation = [instance.tfz,target,prediction]
            if instance.classification == "abstain":
                evaluation.append(abstain)
            else:
                dif = target-prediction
                dif_squared = dif*dif
                evaluation.append(dif_squared) 
                squared_error += dif_squared
                estimations += 1
            stepwise_evaluation.append(evaluation)
        mean_squared_error = squared_error / estimations
        root_mean_squeared_error = math.sqrt(mean_squeared_error)
         


    def evaluate_window(self,event_scores,windowsize,slider,threshold,scoretype):
        #walk through metadict events        
        for event in self.testset_instances.keys():
            print event
            meta_instances = self.testset_instances[event]
            #generate windows
            instance_tfz = []
            for instance in meta_instances:
                instance_tfz.append(([instance],instance.tfz))
            windows = time_functions.extract_sliding_window_instances(instance_tfz,windowsize,slider)
            for scorefile in event_scores[event]:
                classification_info = codecs.open(scorefile,"r","utf-8").readlines()
                classifications = []
                for classification in classification_info:
                    if re.search("==",classification):
                        classifications.append(classification)
                if len(classifications) != len(meta_instances):
                    print "classification and meta do not align, exiting program..."
                    exit()
                for i,line in enumerate(classifications):
                    instance = meta_instances[i]
                    tokens = line.split("==")[1].split("  ")[1].split(" ")
                    if instance.label == tokens[0]:
                        classifications[i] = (instance,tokens[1])
                        instance.set_classification(tokens[1])
                    else:
                        print "error: no label match; exiting program"
                        exit()
                #for each window
                self.testwindows = []
                for window in windows:
                    tfz = window[1]
                    testwindow = Evalset.Instance()
                    testwindow.set_tfz(window[1])
                    testwindow.set_label(window[0][-1].label)
                    print tfz
                    if tfz <= threshold or len(window[0]) == 0:
                        testwindow.set_classification("abstain")
                        self.testwindows.append(testwindow)
                    else:
                        predictions = defaultdict(int)
                        predictiontype = defaultdict(int)
                        for instance in window[0]:
                            if re.search("-",instance.classification):
                                predictiontype["21"] += 1
                                predictions[instance.classification] += 1
                            else:
                                predictiontype[instance.classification] += 1  
                        ranks = sorted(predictiontype, key=predictions.get, reverse = True):
                        if ranks[0] == "21" or predictiontype["21"] == predictiontype[ranks[0]]:
                            estimation = 0
                            tweets = 0
                            for prediction in predictions.keys():
                                estimation += (int(prediction) * predictions[prediction])
                                tweets += predictions[prediction]
                            estimation = str(estimation/tweets)
                            testwindow.set_classification(estimation)
                        else:
                            testwindow.set_classification("abstain")

    def set_instances_lcs(self,labelfile,classificationfile,evaluationtype = "standard",threshold = False):
         
        labels = open(labelfile)
        for line in labels:
            tokens = line.split(" ")
            filename = tokens[0]
            label = tokens[1].strip()
            instance = Evalset.Instance()
            instance.set_label(label)
            instance.set_name(filename)
            self.name_instance[filename] = instance
            self.instances.append(instance) 
   
        classifications = open(classificationfile)
        for line in classifications:
            tokens = line.split("  ")
            filename = tokens[0].strip()
            instance = self.name_instance[filename]
            scores = tokens[1]
            classification_score = scores.split(" ")[0].split(":")
            if not (threshold and re.search(r"\?",classification_score[0])):  
                classification = classification_score[0]
                score = re.sub("\?","",classification_score[1])
                instance.set_classification(classification)
                if classification != instance.label:
                    instance.set_fp()
                instance.set_score(score)
    
    def set_instances_ib(self,metafile,windowsize,slider,threshold):
        meta = codecs.open(metafile,"r","utf-8")
        metaread = meta.readlines()
        meta.close()
        for line in metaread:
            
            tokens = line.split("\t")
            instance = Evalset.Instance()
            instance.set_name = tokens[0]
            instance.set_event = tokens[1]
            instance.set_label = tokens[2]
            instance.set_classification = tokens[3]
            instance.set_tfz = tokens[4]
    
    
    
    
    def set_instances_annotation(self,infile):
        readfile = open(infile,"r").readlines()
        for line in readfile:
            self.instances.append(line.strip())
    
    def set_instances_sparse_meta(self,infile):
        readfile = open(infile,"r").readlines()
        for line in readfile:
            tokens = line.strip().split("\t")
            instance = Evalset.Instance()
            instance.set_name(tokens[0])
            instance.set_event(tokens[1])
            instance.set_label(tokens[2])
            instance.set_classification(tokens[3])
            instance.set_tfz(tokens[4])
            self.name_instance[filename] = instance
            self.instances.append(instance) 
            
    
    def extract_top_fp(self,outfile,classification,top_n,files):
        out_write = open(outfile,"w")
        fp_instances = defaultdict(int)
        for i in self.instances:
            if (i.fp and i.classification == classification):
                fp_instances[i] = i.score
        
        for index,instance in enumerate(sorted(fp_instances, key=fp_instances.get, reverse=True)[:top_n]):
            fileread = open(files + instance.name,"r").readlines()    
            words = []
            for line in fileread:
                term = line.strip()
                if not re.search("_",term):
                    words.append(term)
            text = " ".join(words)
            out_write.write(str(instance.score) + "\t" + text + "\n")    
        out_write.close()
    
    def generate_timebuckets(self,metafiles,eventfiles):
        print "generating event-time hash..."
        # generate event-time hash
        event_time = dict({})
        for eventfile in eventfiles:
            event_time.update(time_functions.generate_event_time_hash(eventfile))

        print "setting tweet_to_event-time..."
        # for each tweet
        for metafile in metafiles:
            metafile = open(metafile,"r")
            for tweet in metafile:
                # link to event
                tokens = tweet.split(" ")
                filename = tokens[0].strip()
                keyterm = tokens[1]
                tweetdate = tokens[2]
                tweettime = tokens[3]
                tweetdatetime = time_functions.return_datetime(tweetdate,tweettime,"vs")
                eventdatetime_begin = event_time[keyterm][0]
                eventdatetime_end = event_time[keyterm][1]
                if (eventdatetime_begin - tweetdatetime) < event_time[keyterm][2]:
                    #print eventdatetime_begin,tweetdatetime
                    try:                        
                        instance = self.name_instance[filename]
                        if not instance.label == "other":
                            self.time_buckets[0].append(instance)
                    except KeyError:
                        continue
                else:
                    eventdatetime_begin = (eventdatetime_begin - event_time[keyterm][2]) + datetime.timedelta(days = 1) 
                    tweetevent_time = time_functions.timerel(eventdatetime_begin,tweetdatetime,"day") * -1
                    #print tweetevent_time
                    if tweetevent_time < 0:
                        try:
                            instance = self.name_instance[filename]
                            #print instance.label,tweetevent_time
                            if not instance.label == "other":
                                self.time_buckets[tweetevent_time].append(instance)
                        except KeyError:
                            continue
                        
        print "sorting instances..."
        #sort instances
        #self.instances = sorted(self.instances,key = lambda x: x.time_to_event)
        #for tte in sorted(self.time_buckets.keys()):
        #    print tte,len(self.time_buckets[tte])
    #def prune_instances_label(self,label):
    #    new_instances = []
    #    for instance in self.instances:
    #        if not instance.label == label:

    
    def plot_instances_time(self,plotfile):
        # generate coordinates
        x = []
        y = []
        ce = evaluation.ClassEvaluation()
        for tte in sorted(self.time_buckets.keys()):
            if tte > -20:
                for instance in self.time_buckets[tte]:
                    #print instance.label,instance.classification
                    ce.append(instance.label,instance.classification)
                #labels = list(set(ce.goals))
                #for label in labels:
                #    if re.search("-",label):
                #        window = re.search("(\d+)-(\d+)",label)
                #        first = int(window.groups()[0]) * - 1
                #        last = int(window.groups()[1]) * -1
                #        if tte <= first and tte >= last:
                #            print label
                #            f1 = ce.fscore(label)
                #    else:
                #        if int(label) * -1 == tte:
                #            f1 = ce.fscore(label)
                f1 = ce.fscore()
                x.append(tte)
                y.append(f1)
        plt.plot(x,y)
        #plt.legend(legend,loc = "upper right",ncol = 2)
        plt.ylabel("Micro-f1 score")
        plt.xlabel("Time-to-event in days")
        #plt.title("\'Micro-f1 score at\' as event time nears")
        plt.savefig(plotfile)
        
                 
    def print_results(self,outfile):
    
        def if_(test, result, alternative):
            """Like C++ and Java's (test ? result : alternative), except
            both result and alternative are always evaluated. However, if
            either evaluates to a function, it is applied to the empty arglist,
            so you can delay execution by putting it in a lambda.
            >>> if_(2 + 2 == 4, 'ok', lambda: expensive_computation())
            'ok'
            """
            if test:
                if callable(result): return result()
                return result
            else:
                if callable(alternative): return alternative()
                return alternative

        def isnumber(x):
            "Is x a number? We say it is if it has a __int__ method."
            return hasattr(x, '__int__')

        def issequence(x):
            "Is x a sequence? We say it is if it has a __getitem__ method."
            return hasattr(x, '__getitem__')

        def print_table_aligned(table, header=None, sep=' ', numfmt='%g'):
            """Print a list of lists as a table, so that columns line up nicely.
            header, if specified, will be printed as the first row.
            numfmt is the format for all numbers; you might want e.g. '%6.2f'.
            (If you want different formats in differnt columns, don't use print_table.)
            sep is the separator between columns."""
            justs = [if_(isnumber(x), 'rjust', 'ljust') for x in table[0]]
            if header:
                table = [header] + table
            table = [[if_(isnumber(x), lambda: numfmt % x, x)  for x in row]
                     for row in table]
            maxlen = lambda seq: max(map(len, seq))
            sizes = map(maxlen, zip(*[map(str, row) for row in table]))
            for row in table:
                for (j, size, x) in zip(justs, sizes, row):
                    print getattr(str(x), j)(size), sep,
                print
    
        out_write = open(outfile,"w")
        rows = [["Class","Precision","Recall","F1","TPR","FPR","AUC","Samples","Classifications","Correct"]]
        ce = proy_evaluation.ClassEvaluation()
        for instance in self.instances:
            ce.append(instance.label,instance.classification)
        for label in sorted(list(set(ce.goals))):
            if not label == "":
                table = [label,str(round(ce.precision(cls=label),2)),str(round(ce.recall(cls=label),2)),str(round(ce.fscore(cls=label),2))]
                table.extend([str(round(ce.tp_rate(cls=label),2)),str(round(ce.fp_rate(cls=label),2)),str(round(auc([0,round(ce.fp_rate(cls=label),2),1], [0,round(ce.tp_rate(cls=label),2),1]),2))])
                table.extend([str((ce.tp[label] + ce.fn[label])),str((ce.tp[label] + ce.fp[label])),str(ce.tp[label])])
                rows.append(table)
        print_table_aligned(rows)
            
    def calculate_interannotator_agreement(self):
        annotator_couples = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))
        annotator_scores = defaultdict(lambda : defaultdict(int))
        for line in self.instances:
            annotations = line.split("\t")
            for i,annotation in enumerate(annotations):
                annotator_scores[i][annotation] += 1
                j = i
                while j+1 < len(annotations):
                    if annotation != annotations[j+1]:
                        annotator_couples[i][j+1]["odd"] += 1
                    else:
                        annotator_couples[i][j+1]["match"] += 1
                    j += 1    
        
        cohens_kappas = []
        for annotator_1 in annotator_couples.keys():
            for annotator_2 in annotator_couples[annotator_1].keys():
                agreement = annotator_couples[annotator_1][annotator_2]["match"] / len(self.instances)
                random = 0
                for answer in annotator_scores[annotator_1].keys():
                    percent_1 = annotator_scores[annotator_1][answer] / len(self.instances)
                    percent_2 = annotator_scores[annotator_2][answer] / len(self.instances)
                    random += (percent_1 * percent_2)
                ck = (agreement - random) / (1 - random)
                cohens_kappas.append(ck)    
        
        cohens_kappa = sum(cohens_kappas) / len(cohens_kappas)
        return round(cohens_kappa,2)

    def return_precisions_annotations(self,doubt,plot = False):
        majority_judgements = defaultdict(list)
        precisions = []
        precision_strengths = defaultdict(int)
        for line in self.instances:
            scores = line.split("\t")
            num_positive = 0
            for score in scores:
                if score == "1":
                    num_positive += 1
                elif score == "2" and not doubt:
                    num_positive += 1
            percentage = num_positive / len(scores)
            majority = int(len(scores) / 2) + 1 
            if percentage >= 0.5:
                for i in range(majority,len(scores)+1):
                    if i <= num_positive:
                        precision_strengths[i] += 1
                        majority_judgements[i].append(1)
                    else:
                        majority_judgements[i].append(0)
            else:
                for i in range(majority,len(scores)+1):
                    majority_judgements[i].append(0) 
                            
        for ps in sorted(precision_strengths.keys()):
            precision_score = precision_strengths[ps] / len(self.instances)
            precisions.append((ps,precision_score))
        
        if plot:
            for strength in sorted(majority_judgements.keys()):
                rank = []
                precision = []
                seen = 0
                correct = 0
                for judgement in majority_judgements[strength]:
                    seen += 1
                    if judgement == 1:
                        correct += 1 
                    rank.append(seen)
                    precision.append(correct/seen)
                plt.plot(rank,precision)
            
            plt.legend(majority_judgements.keys())
            plt.ylabel("precision at rank")
            plt.xlabel("rank")
            plt.savefig(plot)
        
        return precisions

    class Instance():
        
        def __init__(self):
            self.fname = ""
            self.id = ""
            self.label = ""
            self.event = ""
            self.classification = ""
            self.tfz = ""
            self.time_to_event = 0
            self.score = 0
            self.fp = False
            self.past_window = []
    
        def set_name(self,name):
            self.fname = name
            
        def set_id(self,tid):
            self.id = tid
            
        def set_label(self,label):
            self.label = label
        
        def set_classification(self,classification):
            self.classification = classification

        def set_event(self,event):
            self.event = event

        def set_tfz(self,tfz):
            self.tfz = tfz

        def set_score(self,score):
            self.score = score

        def set_fp(self):
            self.fp = True
        def set_time(self,time):
            self.time_to_event = time
            
        def set_past_window(self,instances):
            self.past_window = instances
    
